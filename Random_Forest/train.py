#!/usr/bin/env python3
"""
Train and evaluate a RandomForest classifier on precomputed ResNet18 features.

The script expects feature files generated by data/extract_features.py:
  features/train_features.npz, features/val_features.npz, features/test_features.npz
Each .npz should contain arrays 'X' (features) and 'y' (labels).
"""
import argparse
import json
import random
from pathlib import Path

import joblib
import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
)


def set_seed(seed: int) -> None:
    random.seed(seed)
    np.random.seed(seed)


def load_features(features_dir: Path, split: str):
    data = np.load(features_dir / f"{split}_features.npz")
    return data["X"], data["y"]


def load_class_names(features_dir: Path):
    names_path = features_dir / "class_names.txt"
    if names_path.exists():
        with open(names_path, "r") as f:
            return [line.strip() for line in f if line.strip()]
    return None


def get_display_labels(class_names, num_classes: int):
    if class_names and len(class_names) == num_classes:
        return class_names
    return [str(i) for i in range(num_classes)]


def make_logger(log_path: Path | None):
    log_file = open(log_path, "w") if log_path else None

    def _log(msg: str = ""):
        print(msg)
        if log_file:
            log_file.write(str(msg) + "\n")
            log_file.flush()

    return _log, log_file


def main():
    parser = argparse.ArgumentParser(
        description="Train RandomForest on precomputed CNN features."
    )
    parser.add_argument(
        "--features_dir",
        type=str,
        default="features",
        help="Directory with *_features.npz and class_names.txt.",
    )
    parser.add_argument(
        "--out_dir",
        type=str,
        default="Random_Forest/output",
        help="Where to save the trained model and metrics.",
    )
    parser.add_argument("--n_estimators", type=int, default=500)
    parser.add_argument("--max_depth", type=int, default=None)
    parser.add_argument(
        "--max_features",
        type=str,
        default="sqrt",
        help="Feature subsampling strategy (e.g., 'sqrt', 'log2', None).",
    )
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument(
        "--log_file",
        type=str,
        default=None,
        help="Optional log filename (written inside out_dir) to mirror stdout.",
    )
    args = parser.parse_args()

    set_seed(args.seed)

    features_dir = Path(args.features_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    log_path = out_dir / args.log_file if args.log_file else None
    log, log_file = make_logger(log_path)

    log("Loading features...")
    X_train, y_train = load_features(features_dir, "train")
    X_val, y_val = load_features(features_dir, "val")
    X_test, y_test = load_features(features_dir, "test")

    class_names = load_class_names(features_dir)
    num_classes = len(class_names) if class_names else len(np.unique(y_train))
    display_labels = get_display_labels(class_names, num_classes)
    log(f"Train shape: {X_train.shape}, #classes: {num_classes}")

    estimator_grid = sorted(set([50, 100, 200, args.n_estimators]))
    val_macro_f1_points: list[tuple[int, float]] = []
    model = None

    log("Running n_estimators sweep on validation set for macro-F1...")
    for n_estimators in estimator_grid:
        sweep_model = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=args.max_depth,
            max_features=args.max_features,
            n_jobs=-1,
            class_weight="balanced",
            random_state=args.seed,
        )
        sweep_model.fit(X_train, y_train)
        sweep_val_preds = sweep_model.predict(X_val)
        sweep_macro_f1 = f1_score(
            y_val, sweep_val_preds, average="macro", zero_division=0
        )
        val_macro_f1_points.append((n_estimators, sweep_macro_f1))
        log(f"  n_estimators={n_estimators}: val macro-F1={sweep_macro_f1:.4f}")
        if n_estimators == args.n_estimators:
            model = sweep_model

    sweep_csv_path = out_dir / "n_estimators_vs_val_macro_f1.csv"
    sweep_plot_path = out_dir / "n_estimators_vs_val_macro_f1.png"
    with open(sweep_csv_path, "w") as f:
        f.write("n_estimators,macro_f1\n")
        for n, score in val_macro_f1_points:
            f.write(f"{n},{score:.6f}\n")

    fig, ax = plt.subplots(figsize=(8, 5))
    ax.plot(
        [n for n, _ in val_macro_f1_points],
        [score for _, score in val_macro_f1_points],
        marker="o",
        color="#1f77b4",
    )
    ax.set_title("n_estimators vs Validation Macro-F1")
    ax.set_xlabel("n_estimators")
    ax.set_ylabel("Validation Macro-F1")
    ax.grid(True, alpha=0.3)
    fig.tight_layout()
    fig.savefig(sweep_plot_path, dpi=300)
    plt.close(fig)
    log(f"Saved sweep plot to {sweep_plot_path}")
    log(f"Saved sweep data to {sweep_csv_path}")

    if model is None:
        log("Training RandomForest...")
        model = RandomForestClassifier(
            n_estimators=args.n_estimators,
            max_depth=args.max_depth,
            max_features=args.max_features,
            n_jobs=-1,
            class_weight="balanced",
            random_state=args.seed,
        )
        model.fit(X_train, y_train)
    else:
        log(
            f"Reusing model trained with n_estimators={args.n_estimators} "
            "from sweep for final evaluation."
        )

    log("Evaluating...")
    val_preds = model.predict(X_val)
    test_preds = model.predict(X_test)
    val_acc = accuracy_score(y_val, val_preds)
    test_acc = accuracy_score(y_test, test_preds)
    val_macro_f1 = f1_score(y_val, val_preds, average="macro", zero_division=0)
    test_macro_f1 = f1_score(y_test, test_preds, average="macro", zero_division=0)
    log(f"Validation Accuracy: {val_acc:.4f}")
    log(f"Test Accuracy:       {test_acc:.4f}")
    log(f"Validation Macro-F1: {val_macro_f1:.4f}")
    log(f"Test Macro-F1:       {test_macro_f1:.4f}")

    report = classification_report(
        y_test, test_preds, target_names=class_names, zero_division=0
    )
    log("\nTest Classification Report:\n" + report)

    cm = confusion_matrix(y_test, test_preds, labels=range(num_classes))
    cm_plot_path = out_dir / "confusion_matrix_test.png"
    cm_csv_path = out_dir / "confusion_matrix_test.csv"
    np.savetxt(cm_csv_path, cm, fmt="%d", delimiter=",")
    fig_side = min(24, max(10, num_classes * 0.4))
    fig, ax = plt.subplots(figsize=(fig_side, fig_side))
    sns.heatmap(
        cm,
        cmap="Blues",
        annot=False,
        cbar=True,
        square=True,
        xticklabels=display_labels,
        yticklabels=display_labels,
        linewidths=0.2,
        linecolor="white",
        ax=ax,
    )
    ax.set_xlabel("Predicted Label")
    ax.set_ylabel("True Label")
    ax.set_title("Confusion Matrix (Test Set)")
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    fig.tight_layout()
    fig.savefig(cm_plot_path, dpi=600, bbox_inches="tight")
    plt.close(fig)
    log(f"Saved confusion matrix plot to {cm_plot_path}")
    log(f"Saved confusion matrix values to {cm_csv_path}")

    per_class_f1 = f1_score(
        y_test, test_preds, labels=range(num_classes), average=None, zero_division=0
    )
    f1_csv_path = out_dir / "per_class_f1_scores.csv"
    f1_plot_path = out_dir / "per_class_f1_bar.png"
    with open(f1_csv_path, "w") as f:
        f.write("class,f1_score\n")
        for label, score in zip(display_labels, per_class_f1):
            f.write(f"{label},{score:.6f}\n")
    fig, ax = plt.subplots(figsize=(10, 6))
    ax.bar(display_labels, per_class_f1, color="#4C72B0")
    ax.set_ylim(0, 1)
    ax.set_ylabel("F1-score")
    ax.set_title("Per-class F1-score (Test Set)")
    plt.xticks(rotation=45, ha="right")
    fig.tight_layout()
    fig.savefig(f1_plot_path, dpi=300)
    plt.close(fig)
    log(f"Saved per-class F1 scores to {f1_csv_path}")
    log(f"Saved per-class F1 bar plot to {f1_plot_path}")

    importances = model.feature_importances_
    fi_csv_path = out_dir / "feature_importances.csv"
    with open(fi_csv_path, "w") as f:
        f.write("feature_index,importance\n")
        for idx, imp in enumerate(importances):
            f.write(f"{idx},{imp:.8f}\n")
    top_k = min(20, importances.shape[0])
    top_indices = np.argsort(importances)[::-1][:top_k]
    top_importances = importances[top_indices]
    top_labels = [f"feature_{i}" for i in top_indices]
    labels_for_plot, importances_for_plot = zip(
        *reversed(list(zip(top_labels, top_importances)))
    )
    fig, ax = plt.subplots(figsize=(10, 7))
    ax.barh(range(top_k), importances_for_plot, color="#2ca02c")
    ax.set_yticks(range(top_k))
    ax.set_yticklabels(labels_for_plot)
    ax.set_xlabel("Importance")
    ax.set_title(f"Top {top_k} Feature Importances")
    fig.tight_layout()
    fi_plot_path = out_dir / "feature_importance_top20.png"
    fig.savefig(fi_plot_path, dpi=300)
    plt.close(fig)
    log(f"Saved feature importances to {fi_csv_path}")
    log(f"Saved feature importance plot to {fi_plot_path}")

    model_path = out_dir / "random_forest.joblib"
    metrics_path = out_dir / "metrics.json"
    report_path = out_dir / "classification_report.txt"

    joblib.dump(model, model_path)
    with open(metrics_path, "w") as f:
        json.dump(
            {
                "val_accuracy": val_acc,
                "test_accuracy": test_acc,
                "val_macro_f1": val_macro_f1,
                "test_macro_f1": test_macro_f1,
                "n_estimators": args.n_estimators,
                "max_depth": args.max_depth,
                "max_features": args.max_features,
                "seed": args.seed,
            },
            f,
            indent=2,
        )
    with open(report_path, "w") as f:
        f.write(report)

    log(f"Saved model to {model_path}")
    log(f"Saved metrics to {metrics_path}")
    log(f"Saved classification report to {report_path}")

    if log_file:
        log_file.close()


if __name__ == "__main__":
    main()
